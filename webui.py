import gradio as gr
from transformers import AutoTokenizer, pipeline
import torch

model = "google/gemma-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = pipeline(
    "text-generation",
    model=model,
    model_kwargs={
        "torch_dtype": torch.float16,
    },
    device="cuda",
)

chat_history = []
def chatbot(input_text, max_new_tokens=1250, temperature=0.7, top_k=50, top_p=0.95):
    messages = [{"role": "user", "content": input_text}]
    prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    outputs = pipeline(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p
    )
    generated_text = outputs[0]["generated_text"][len(prompt):]
    chat_history.append({"user":input_text, "gemgpt": generated_text})
    return generated_text



with gr.Blocks() as iface:

    gr.Markdown("Talk to GemGPT,Powered by a 2 billion parameter Gemma Model")
    with gr.Row():
        gr.Markdown("Output Generated by GemGPT-2B-IT Model.")
    with gr.Row():
        outputs=gr.Textbox(lines=15, label="Output" ,value="")
    with gr.Row():
        gr.Markdown("Input your text and click Run to generate a response.")
    with gr.Row():
        inputs=gr.Textbox(lines=2, label="Input")

    with gr.Row():
        generate=gr.Button("Run")
    with gr.Row():
        gr.Markdown("Adjust the parameters to control the model's output.")
    with gr.Row():
        tokens=gr.Slider(minimum=50, maximum=2000, label="Max New Tokens", value=1250)
    with gr.Row():
        temp=gr.Slider(minimum=0.0, maximum=1.0, label="Temperature", value=0.7)
    with gr.Row():
        top_k=gr.Slider(minimum=1, maximum=100, label="Top K", value=50)
    with gr.Row():
        top_p=gr.Slider(minimum=0.0, maximum=1.0, label="Top P", value=0.95)
    generate.click(fn=chatbot, inputs=[inputs, tokens, temp, top_k, top_p], outputs=outputs)

iface.launch()